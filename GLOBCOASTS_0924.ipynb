{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULES IMPORTATION\n",
    "import math\n",
    "from math import *\n",
    "import datetime\n",
    "from datetime import *\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from cmcrameri import cm\n",
    "from pyproj import Geod\n",
    "\n",
    "print(\"#################################\")\n",
    "print(\"# - Importing modules...\")\n",
    "\n",
    "try:\n",
    "    print(\"# - Importing numpy\")\n",
    "    import numpy as np\n",
    "    from numpy import *\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing numpy\")\n",
    "try:\n",
    "    print(\"# - Importing pandas\")\n",
    "    import pandas as pd\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing pandas\")\n",
    "    \n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing geopandas\")\n",
    "    \n",
    "try:\n",
    "    print(\"# - Importing pylab\")\n",
    "    import pylab\n",
    "    from pylab import *\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing pylab\")\n",
    "try:\n",
    "    print(\"# - Importing matplotlib\")\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as colors\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    import matplotlib.lines as mlines\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "    import matplotlib.font_manager as fm\n",
    "    from cartopy import crs as ccrs, feature as cfeature\n",
    "    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing matplotlib\")\n",
    "try:\n",
    "    print(\"# - Importing scipy\")\n",
    "    import scipy\n",
    "    from scipy import interpolate\n",
    "    from scipy.interpolate import interp1d, interp2d\n",
    "    from scipy.interpolate import griddata\n",
    "    from scipy.stats import *\n",
    "    import scipy.io as mio\n",
    "    from scipy.spatial import distance\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing scipy\")\n",
    "try:\n",
    "    print(\"# - Importing gc\")\n",
    "    import gc\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing gc\")\n",
    "try:\n",
    "    print(\"# - Importing socket\")\n",
    "    import socket\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing socket\")\n",
    "try:\n",
    "    print(\"# - Importing getpass\")\n",
    "    import getpass\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing getpass\")\n",
    "try:\n",
    "    import configparser\n",
    "    from configparser import *\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing ConfigParser\")\n",
    "try:\n",
    "    print(\"# - Importing statistics\")\n",
    "    import statistics\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing statistics\")\n",
    "\n",
    "try:\n",
    "    print(\"# - Importing h5py\")\n",
    "    import h5py\n",
    "except:\n",
    "    raise ImportError(\"  ERROR importing h5py\")\n",
    "try:\n",
    "    print(\"# -Importing xarray\")\n",
    "    import xarray as xr\n",
    "except:\n",
    "        raise ImportError(\"  ERROR importing xarray\")\n",
    "        \n",
    "try:\n",
    "    print(\"# - Importing cartopy\")\n",
    "    import cartopy\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing cartopy\")\n",
    "\n",
    "try:\n",
    "    print(\"# - Importing globe\")\n",
    "    from global_land_mask import globe\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing globe\")\n",
    "    \n",
    "try:\n",
    "    print(\"# - Importing netcdf4\")\n",
    "    from netCDF4 import Dataset\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing netcdf\")\n",
    "    \n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f74d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH DEFINITION TO TOOLBOXES DIRECTORY\n",
    "try:\n",
    "    print(\"# - Importing main toolboxes path...\")\n",
    "    global PTOOL\n",
    "    # If path to TOOLBOXES is in you environment variables\n",
    "    PTOOL = 'C:/Users/arias/ownCloud/GLOBCOAST/FUNCTION/'\n",
    "except:\n",
    "    raise ImportError(\" ERROR path is not defined \")\n",
    "\n",
    "# IMPORT OF TOOLBOXES\n",
    "# (must respect dependancies between toolboxes)\n",
    "\n",
    "print(\"# - Importing personal toolboxes...\")\n",
    "try:\n",
    "    sys.path.append(PTOOL)\n",
    "except:\n",
    "    raise ImportError(\" ERROR adding TOOLBOX PATH\")\n",
    "    \n",
    "#  MAIN TOOLBOX\n",
    "try:\n",
    "    import function as FUNC    # toolbox where are all the functions I create for the model\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing FUNC\")\n",
    "    \n",
    "try:\n",
    "    import PYSTATS as PYSTATS    # toolbox where are all the functions I create for the model\n",
    "except:\n",
    "    raise ImportError(\" ERROR importing FUNC\")\n",
    "    \n",
    "print(\"# - Toolboxes are OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_climatologie_saisonniere(dataframes, date_col='date'):\n",
    "    climatologies_saisonnieres = {}\n",
    "    \n",
    "    for name, df in dataframes.items():\n",
    "        # Assurez-vous que la colonne de date est en index si ce n'est pas déjà le cas\n",
    "        if date_col in df.columns:\n",
    "            df.set_index(date_col, inplace=True)\n",
    "        \n",
    "        # Extraire le mois et ajouter une colonne 'Mois'\n",
    "        df['Mois'] = df.index.month\n",
    "        \n",
    "        # Calculer la moyenne par mois\n",
    "        climatologies_saisonnieres[name] = df.groupby('Mois').mean()\n",
    "    \n",
    "    return climatologies_saisonnieres\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='same')\n",
    "\n",
    "# Fonction pour calculer la distance entre deux points de latitude/longitude\n",
    "def haversine(coord1, coord2):\n",
    "    # Convertir les degrés en radians\n",
    "    lat1, lon1 = radians(coord1[0]), radians(coord1[1])\n",
    "    lat2, lon2 = radians(coord2[0]), radians(coord2[1])\n",
    "\n",
    "    # Calcul de la différence entre les points\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = 6371 * c  # Rayon de la Terre en kilomètres\n",
    "    return distance\n",
    "\n",
    "# Algorithme du plus proche voisin\n",
    "def nearest_neighbor(coords):\n",
    "    if not coords:\n",
    "        return []\n",
    "\n",
    "    path = [coords.pop(0)]  # Commence avec le premier point\n",
    "    while coords:\n",
    "        # Trouve le point le plus proche du dernier point ajouté au chemin\n",
    "        next_point = min(coords, key=lambda x: haversine(path[-1], x))\n",
    "        path.append(next_point)\n",
    "        coords.remove(next_point)\n",
    "\n",
    "    return path\n",
    "\n",
    "def lonlat2xy(lon, lat, lon_0, lat_0):\n",
    "    \"\"\"\n",
    "    lonlat2xy converts pairs of longitude and latitude into Cartesian\n",
    "    coordinates x and y with lon_0 and lat_0 representing the coordinate\n",
    "    system origin. \n",
    "    \"\"\"\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    lon_0_ar, lat_0_ar = np.full_like(lon, lon_0), np.full_like(lat, lat_0)\n",
    "\n",
    "    # Calculate y coordinates \n",
    "    yd = np.where(lat < lat_0, -1, 1) * geod.inv(lon_0_ar, lat_0_ar, lon_0_ar, lat)[2]\n",
    "\n",
    "    # Calculate x coordinates \n",
    "    xd = np.where(lon < lon_0, -1, 1) * geod.inv(lon_0_ar, lat_0_ar, lon, lat_0_ar)[2]\n",
    "\n",
    "    return xd, yd\n",
    "\n",
    "def xy2lonlat(x, y, lon_0, lat_0):\n",
    "    \"\"\"\n",
    "    xy2lonlat converts pairs of Cartesian coordinates x and y back to\n",
    "    longitude and latitude with lon_0 and lat_0 representing the coordinate\n",
    "    system origin. Longitude and latitude are returned in degrees.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Geod object for distance calculation\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "    lon_0_ar, lat_0_ar = np.full_like(x, lon_0), np.full_like(y, lat_0)\n",
    "\n",
    "    # Calculate longitudes\n",
    "    lon = geod.fwd(lon_0_ar, lat_0_ar, np.full_like(x, 90), x)[0]\n",
    "\n",
    "    # Calculate latitudes\n",
    "    lat = geod.fwd(lon_0_ar, lat_0_ar, np.zeros_like(y), y)[1]\n",
    "\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def calculer_azimut(lat, lon):\n",
    "    \"\"\"\n",
    "    Calcule l'azimut entre les points successifs spécifiés par leurs coordonnées de latitude et de longitude.\n",
    "\n",
    "    Parameters:\n",
    "    lat_ok (array-like): Tableau des latitudes des points.\n",
    "    lon_ok (array-like): Tableau des longitudes des points.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Tableau des azimuts entre les points successifs.\n",
    "    \"\"\"\n",
    "    alpha = np.zeros(len(lon) - 1)\n",
    "\n",
    "    for i in range(len(lon) - 1):\n",
    "        lat1 = np.radians(lat[i])\n",
    "        lat2 = np.radians(lat[i+1])\n",
    "        d_lon = np.radians(lon[i+1] - lon[i])\n",
    "\n",
    "        x_azimuth = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(d_lon)\n",
    "        y_azimuth = np.sin(d_lon) * np.cos(lat2)\n",
    "\n",
    "        alpha[i] = np.arctan2(y_azimuth, x_azimuth)\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def calculer_normale_orientee(alpha, Dir):\n",
    "    \"\"\"\n",
    "    Calcule les normales orientées en fonction de Dir.\n",
    "\n",
    "    Parameters:\n",
    "    alpha (numpy.ndarray): Tableau des azimuts.\n",
    "    Dir (numpy.ndarray): Tableau des directions de vagues pour chaque point (ou zone).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Tableau des normales orientées.\n",
    "    \"\"\"\n",
    "    # Convertir les angles en radians si nécessaire\n",
    "    Dir_rad = np.radians(Dir)\n",
    "\n",
    "    # Calcul des normales perpendiculaires\n",
    "    normal_1 = (alpha + np.pi/2) % (2*np.pi)\n",
    "    normal_2 = (alpha - np.pi/2) % (2*np.pi)\n",
    "    normal_finale = np.zeros_like(alpha)\n",
    "\n",
    "    for i in range(len(normal_1)):\n",
    "        if abs(normal_1[i] - Dir_rad[i]) < abs(normal_2[i] - Dir_rad[i]):\n",
    "            normal_finale[i] = normal_1[i]\n",
    "        else:\n",
    "            normal_finale[i] = normal_2[i]\n",
    "\n",
    "    return normal_finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILES IMPORT\n",
    "\n",
    "print(\"# - Retrieving Main files...\")\n",
    "pathin = 'C:/Users/arias/ownCloud/DATAS/Fichier_seul/' \n",
    "pathout = 'C:/Users/arias/ownCloud/GLOBCOAST/FIGURES/'\n",
    "# ---------------------------------------------------------------------------------------\n",
    "print(\"# - Uploading files ...\")\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# BQART DATA\n",
    "Bqart_file = 'Sorties_Bqart.txt'\n",
    "BQART_brut   = np.loadtxt(pathin+ Bqart_file, delimiter=';')\n",
    "BQART_m3     = BQART_brut[:,0][:8841]/2650 #on divise par la densité du sable --> m3/an\n",
    "BQART        = BQART_m3/12            # m3/yr --> m3/month\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# SEADATAS FROM JULIEN\n",
    "SEADATA = xr.open_dataset(pathin+'SEADATA_14140pts_1993_2019-analysed.nc',engine='netcdf4')\n",
    "SEADATA_lon = SEADATA['lon'].values\n",
    "SEADATA_lat = SEADATA['lat'].values\n",
    "SEADATA_Hs  = SEADATA['Hs_mounthly'].values\n",
    "SEADATA_Tp  = SEADATA['Tp_mounthly'].values\n",
    "SEADATA_Dir = SEADATA['dir_mounthly'].values\n",
    "SEADATA_SLA = SEADATA['sla_detrend'].values\n",
    "SEADATA_DAC = SEADATA['dac_detrend'].values\n",
    "SEADATA_RIV = SEADATA['rivdis_mounthly'].values\n",
    "lon = SEADATA_lon[:8841]\n",
    "lat = SEADATA_lat[:8841]\n",
    "Hs  = SEADATA_Hs[84:,:8841]\n",
    "Tp  = SEADATA_Tp[84:,:8841]\n",
    "Dir = SEADATA_Dir[84:,:8841]\n",
    "SLA = SEADATA_SLA[84:,:8841]\n",
    "DAC = SEADATA_DAC[84:,:8841]\n",
    "RiverD = SEADATA_RIV[84:,:8841]\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "TIDE = scipy.io.loadmat('C:/Users/arias/ownCloud/GLOBCOAST/DATAS/Tide_Glob.mat')\n",
    "Marnage = TIDE['Tide_max'][0][:8841]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# # Validation file  (8841(position) x 444(time) --> 1984 to 2020 monthly) -> select 1993 to 2019\n",
    "Validation     = scipy.io.loadmat(pathin+'Shorelines_global_20231101_shift.mat')\n",
    "Xshores_val    = Validation['X_safe'][:,192:-12]\n",
    "latX = Validation['latX'][:]\n",
    "lonX = Validation['lonX'][:]\n",
    "\n",
    "Xshores_VALIDATION = np.transpose(Xshores_val)\n",
    "print(\"# - All files are upload...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d418fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# - Initializing constant values...\")\n",
    "# Median grain size [m]\n",
    "d50 = 10e-3\n",
    "\n",
    "# Sand porosity\n",
    "poro = 0.4\n",
    "\n",
    "# Sand density [kg/m3]\n",
    "rohs      = 2650 \n",
    "\n",
    "# Water density [kg/m3]\n",
    "roh       = 1000 \n",
    "\n",
    "# Slope hypothesis \n",
    "#slope = 0.05\n",
    "\n",
    "# gravitationnal acceleration [m/s2]\n",
    "g = 9.81\n",
    "\n",
    "# active profil heigh --> in the future it will be DOC value +berme value [m] we can also use Dc as \n",
    "\n",
    "DoC = 10 \n",
    "\n",
    "# Earth radius [m]\n",
    "R = 6371000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(Marnage)):\n",
    "        if str(Marnage[i]) == 'nan':\n",
    "            Marnage[i]=Marnage[i-1]\n",
    "            \n",
    "print(\"# - Marnage is ok\")\n",
    "           \n",
    "for ligne in RiverD:\n",
    "    for i in range(len(ligne)):\n",
    "        # Vérifier si l'élément est \"nan\" et le remplacer par 0\n",
    "        if str(ligne[i]) == 'nan':\n",
    "            ligne[i] = 0\n",
    "\n",
    "QrivD = FUNC.RIVDIS(RiverD,BQART)\n",
    "for ligne in QrivD:\n",
    "    if str(ligne) == 'nan':\n",
    "        print(ligne)\n",
    "        \n",
    "print(\"# - QrivD is ok\")\n",
    "\n",
    "L_SU = (g/(2*np.pi))*Tp**2\n",
    "\n",
    "beta       = 0.12 * ((np.sqrt(2*np.pi*d50*L_SU))/(Hs * (1+(Marnage/Hs))))**(1/2)\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    for j in range(len(beta[0])):\n",
    "        if str(beta[i,j]) == 'nan':\n",
    "            print(i,j)\n",
    "            \n",
    "print(\"# - foreshore slopes is ok \")\n",
    "\n",
    "# Set up des vagues [m] \n",
    "SU   = 0.35*beta*np.sqrt(Hs*L_SU)\n",
    "\n",
    "print(\"# - Set up is ok\")\n",
    "\n",
    "TWL  = SLA + DAC+ SU\n",
    "\n",
    "print(\"# - TWL is ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(Marnage, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=Marnage, s=10)\n",
    "ax2.hist(Marnage.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='5th Percentile')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='95th Percentile')\n",
    "\n",
    "ax2.set_xlabel('Tide')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Marnage max [m]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Marnage max over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(RiverD, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=(np.mean(RiverD,axis=0)), s=10)\n",
    "ax2.hist(RiverD.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='5th Percentile')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='95th Percentile')\n",
    "\n",
    "ax2.set_xlabel('River discharge')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='River discharge [m3/month]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('River discharge from ISBA over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(BQART, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=BQART, s=10)\n",
    "ax2.hist(BQART.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='5th Percentile')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='95th Percentile')\n",
    "\n",
    "ax2.set_xlabel('BQART')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='BQART [m3/month]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('BQART over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_percentiles = np.percentile(QrivD, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(QrivD, axis=0), s=10)\n",
    "ax2.hist(QrivD.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "ax2.set_xlabel('River \"solid\" discharge (BQART+ISBA) [m3/month]',fontsize=16)\n",
    "ax2.set_ylabel('Fréquence',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='[m3/month]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('MeanRiver \"solid\" discharge over the period 2000-2019')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_percentiles = np.percentile(Hs, [5, 95])\n",
    "\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(Hs, axis=0), s=10)\n",
    "ax2.hist(Hs.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "ax2.set_xlabel('Hs [m]', fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.legend(fontsize=16)\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Wave height [m]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean wave height over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_percentiles = np.percentile(Tp, [5, 95])\n",
    "\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(Tp, axis=0), s=10)\n",
    "ax2.hist(Tp.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "ax2.set_xlabel('Tp [s]', fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.legend(fontsize=16)\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Peak period [s]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean peak period over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_percentiles = np.percentile(Dir, [5, 95])\n",
    "\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(Dir, axis=0), s=10)\n",
    "ax2.hist(Dir.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "ax2.set_xlabel('Dir [°]', fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.legend(fontsize=16)\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Wave direction [°]')\n",
    "\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean wave direction over the period 2000-2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(SU, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(SU, axis=0), s=10)\n",
    "ax2.hist(SU.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel('Set-Up [m]',fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Wave set-up with slope corrected by tide [m]')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean wave set-up over the period 2000-2019')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(DAC, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(DAC, axis=0), s=10)\n",
    "ax2.hist(DAC.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel('Run-up [m]',fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='DAC [m]')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean DAC over the period 2000-2019')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(SLA, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(SLA, axis=0), s=10)\n",
    "ax2.hist(SLA.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel('Sea level anomaly [m]',fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='mean SLA [m]')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean of SLA over the period 2000-2019')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c940f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(np.tan(beta), [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(np.tan(beta), axis=0), s=10)\n",
    "ax2.hist(np.tan(beta).flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='5th Percentile')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='95th Percentile')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel('Foreshore slope')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='slope')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean slope over the period 2000-2019')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les percentiles globaux entre toutes les positions et tous les temps\n",
    "global_percentiles = np.percentile(TWL, [5, 95])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Percentiles globaux (5%, 95%):\", global_percentiles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=np.mean(TWL, axis=0), s=10)\n",
    "ax2.hist(TWL.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel(\"Total water level [m]\",fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Total water level with slope corrected[m]')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "ax1.set_title('Mean total water level over the period 1993-2016')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7e7e1",
   "metadata": {},
   "source": [
    "# ZONE DELIMITATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_deb, index_fin = FUNC.trouver_indice_section(lon, lat,0.55)\n",
    "sections_fusionnees,indices_fusionnes = FUNC.fusionner_sections(index_deb, index_fin, lon, lat,0.55)\n",
    "paires_filtrees = [(i, j) for i, j in indices_fusionnes if abs(i - j) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la figure et de l'axe avec projection\n",
    "fig, ax1 = plt.subplots(figsize=(14, 15), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajout des caractéristiques de la carte\n",
    "ax1.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax1.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax1.set_extent([-180, 180, -65, 65], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax1.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax1.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "for i, section in enumerate(sections_fusionnees):\n",
    "    plt.plot(section[:, 0], section[:, 1], label=f'Section {i + 1}')\n",
    "plt.scatter(lon[3505:3543],lat[3505:3543])\n",
    "plt.legend()\n",
    "    # Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df010de1",
   "metadata": {},
   "source": [
    "CALIBRATION SUR TWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0acb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xshores_VALIDATION = Xshores_val.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_len = len(lon)\n",
    "sigma = 1.0\n",
    "Nmonth = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list= pd.date_range('2000-1-1','2019-12-31', freq='M').strftime(\"%Y-%m-%d\")\n",
    "date = pd.DatetimeIndex(date_list)\n",
    "num_dates = len(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_smooth(data, sigma):\n",
    "    smoothed_data = np.apply_along_axis(lambda m: gaussian_filter1d(m, sigma=sigma), axis=0, arr=data)\n",
    "    return smoothed_data    \n",
    "\n",
    "def objective_function(c, TWL, beta, Xshores_VALIDATION, lon_index, sigma, date, Nmonth):\n",
    "    X = np.zeros(Nmonth)\n",
    "    for t in range(1, Nmonth):\n",
    "        dx_hydro  = - (TWL[t, lon_index] - TWL[t - 1, lon_index]) / (c * np.tan(beta[t, lon_index]))\n",
    "        dx_morpho = -((1 / (c * np.tan(beta[t, lon_index]))) - (1 / (c * np.tan(beta[t - 1, lon_index])))) \n",
    "        X[t] = X[t - 1] + (dx_hydro + dx_morpho)\n",
    "    \n",
    "    # Detrend des signaux\n",
    "    X_detrend = np.apply_along_axis(detrend_linear,0,X)\n",
    "    val_detrend = np.apply_along_axis(detrend_linear,0,Xshores_VALIDATION[:, lon_index])\n",
    "\n",
    "    # Filtre gaussien\n",
    "    X_detrend2 = gaussian_smooth(X_detrend, sigma)\n",
    "    val_detrend2 = gaussian_smooth(val_detrend, sigma=sigma)\n",
    "\n",
    "    # Calcul de la climatologie saisonnière\n",
    "    climato_X_TOT = pd.DataFrame(X_detrend2, index=date)\n",
    "    climato_val_TOT = pd.DataFrame(val_detrend2, index=date)\n",
    "    \n",
    "    dataframes_TOT = {\n",
    "        'climato_X': climato_X_TOT,\n",
    "        'climato_val': climato_val_TOT\n",
    "    }\n",
    "    \n",
    "    climatologies_saisonnieres_TOT = calculer_climatologie_saisonniere(dataframes_TOT)\n",
    "    climato_saisonniere_X_TOT = climatologies_saisonnieres_TOT['climato_X']\n",
    "    climato_saisonniere_val_TOT = climatologies_saisonnieres_TOT['climato_val']\n",
    "    \n",
    "    # Calculer l'écart type\n",
    "    std_obs = climato_saisonniere_val_TOT.std(axis=0)\n",
    "    std_climato_X = climato_saisonniere_X_TOT.std(axis=0)\n",
    "    \n",
    "    # Comparaison des écarts types\n",
    "    std_compar = abs(std_obs - std_climato_X)\n",
    "\n",
    "    # Ajout d'un contrôle de corrélation\n",
    "    correlation = climato_saisonniere_X_TOT.corrwith(climato_saisonniere_val_TOT).mean()\n",
    "    if correlation < 0:\n",
    "        std_compar += abs(correlation)  # Pénaliser les corrélations négatives\n",
    "\n",
    "    return std_compar\n",
    "\n",
    "# Fonction pour visualiser les résultats\n",
    "def plot_results(results, date):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for (lon_index, c_results) in results.items():\n",
    "        for c, (climato_X_TOT, climato_val_TOT) in c_results.items():\n",
    "            ax.plot(climato_X_TOT.index, climato_X_TOT, label=f'Lon={lon_index}, c={c}')\n",
    "        ax.plot(climato_val_TOT.index, climato_val_TOT, linestyle='--', label=f'Validation {lon_index}')\n",
    "    \n",
    "    # Ajouter les étiquettes et la légende\n",
    "    ax.set_xlabel('Month', fontsize=16)\n",
    "    ax.set_xticks(np.arange(1, 13, step=1))\n",
    "    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Définir une gamme de valeurs pour c\n",
    "c_values = [0.00000000000001, 0.0001,1]  # Ajustez selon vos besoins\n",
    "\n",
    "# Dictionnaire pour stocker les résultats\n",
    "results = {}\n",
    "best_c_values = []\n",
    "# Optimisation pour chaque position de longitude\n",
    "for lon_index in range(lon_len):  # Assurez-vous que lon_index parcourt toutes les positions de longitude\n",
    "    best_c = None\n",
    "    best_result = None\n",
    "    for c in c_values:\n",
    "        # Effectuer la minimisation pour chaque valeur de c\n",
    "        result = minimize(objective_function, x0=c, args=(TWL, beta, Xshores_VALIDATION, lon_index, sigma, date, Nmonth), bounds=[(0.00000000000001,1)])\n",
    "        \n",
    "        # Choisir la meilleure valeur de c en fonction de la fonction objectif\n",
    "        if best_result is None or result.fun < best_result:\n",
    "            best_result = result.fun\n",
    "            best_c = result.x[0]\n",
    "            \n",
    "    best_c_values.append(best_c)\n",
    "    \n",
    "    # Afficher la meilleure valeur de c pour chaque longitude\n",
    "#     print(f'Longitude Index: {lon_index}, Best c: {best_c}')\n",
    "\n",
    "    # Calculer les résultats pour le meilleur c\n",
    "    X = np.zeros(Nmonth)\n",
    "    for t in range(1, Nmonth):\n",
    "        dx_hydro  = - (TWL[t, lon_index] - TWL[t - 1, lon_index]) / (best_c * np.tan(beta[t, lon_index]))\n",
    "        dx_morpho = -((1 / (best_c * np.tan(beta[t, lon_index]))) - (1 / (best_c * np.tan(beta[t - 1, lon_index])))) \n",
    "        X[t] = X[t - 1] + (dx_hydro + dx_morpho)\n",
    "    \n",
    "    X_detrend = detrend_linear(X)\n",
    "    val_detrend = detrend_linear(Xshores_VALIDATION[:, lon_index])\n",
    "\n",
    "    X_detrend2 = gaussian_filter1d(X_detrend, sigma=sigma)\n",
    "    val_detrend2 = gaussian_filter1d(val_detrend, sigma=sigma)\n",
    "\n",
    "    climato_X_TOT = pd.DataFrame(X_detrend2, index=date)\n",
    "    climato_val_TOT = pd.DataFrame(val_detrend2, index=date)\n",
    "\n",
    "    dataframes_TOT = {\n",
    "        'climato_X': climato_X_TOT,\n",
    "        'climato_val': climato_val_TOT\n",
    "    }\n",
    "    \n",
    "    climatologies_saisonnieres_TOT = calculer_climatologie_saisonniere(dataframes_TOT)\n",
    "    climato_saisonniere_X_TOT = climatologies_saisonnieres_TOT['climato_X']\n",
    "    climato_saisonniere_val_TOT = climatologies_saisonnieres_TOT['climato_val']\n",
    "    \n",
    "    # Stocker les résultats avec la meilleure valeur de c\n",
    "    results[lon_index] = {best_c: (climato_saisonniere_X_TOT, climato_saisonniere_val_TOT)}\n",
    "\n",
    "# Visualiser les résultats\n",
    "# plot_results(results, date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('best_c_values_141124.npy', best_c_values)\n",
    "best_c_values = np.load('best_c_values_141124.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la figure et de l'axe avec projection\n",
    "fig, ax1 = plt.subplots(figsize=(14, 15), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajout des caractéristiques de la carte\n",
    "ax1.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax1.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax1.set_extent([-180, 180, -65, 65], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax1.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax1.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Scatter plot\n",
    "sc1 = ax1.scatter(lon, lat, c=best_c_values, s=10,cmap='RdBu', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc1, ax=ax1, orientation='vertical', fraction=0.017, pad=0.04, shrink=0.8)\n",
    "cbar.set_label('c', fontsize=16)\n",
    "\n",
    "# Ajustement de la taille des labels de la colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "slop_c = best_c_values * np.tan(beta)\n",
    "slope_c_med = np.median(slop_c,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_percentiles = np.percentile(slope_c_med, [5, 95])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))  # Correction ici\n",
    "\n",
    "sc = ax1.scatter(lon, lat, c=slope_c_med, s=10)\n",
    "\n",
    "# Ajoutez une colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax1, label='Median (c*tan(slope)')\n",
    "\n",
    "# Assurez-vous d'utiliser le bon axe pour les étiquettes et le titre\n",
    "ax1.set_xlabel('Longitude (°)')\n",
    "ax1.set_ylabel('Latitude (°)')\n",
    "\n",
    "ax2.hist(slope_c_med.flatten(), bins=50, alpha=0.7, color='grey', label='Distribution')\n",
    "ax2.axvline(global_percentiles[0], color='red', linestyle='dashed', linewidth=1, label='Percentile 5%')\n",
    "ax2.axvline(global_percentiles[1], color='green', linestyle='dashed', linewidth=1, label='Percentile 95%')\n",
    "\n",
    "# Correction ici\n",
    "ax2.set_xlabel(\"Median c*tan(beta)\",fontsize=16)\n",
    "ax2.set_ylabel('Frequency',fontsize=16)\n",
    "ax2.legend(fontsize=16)\n",
    "ax2.tick_params(labelsize=13)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_indices = []\n",
    "r_dKAMP = []\n",
    "r_dTWL = []\n",
    "r_Ls = []\n",
    "r_alpha = []\n",
    "r_normal_finale = []\n",
    "r_angle_inci_test = []\n",
    "\n",
    "r_dx_CS_Hydro = []\n",
    "r_dx_CS_MorphoLST = []\n",
    "r_dx_CS_MorphoTOT = []\n",
    "r_dx_CS_MorphoXshore = []\n",
    "r_dx_CS_total = []\n",
    "\n",
    "r_X_CS_LST = []\n",
    "r_X_CS_River = []\n",
    "r_X_CS_MorphoTOT = []\n",
    "r_X_CS_Hydro = []\n",
    "r_X_CS_Xshore = []\n",
    "r_X_CS_total = []\n",
    "\n",
    "r_Lon = []\n",
    "r_Lat = []\n",
    "r_Lon_m = []\n",
    "r_Lat_m = []\n",
    "r_DLon = []\n",
    "r_DLat = []\n",
    "r_dLon = []\n",
    "r_dLat = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, section in tqdm(enumerate(paires_filtrees)):\n",
    "    index_deb = section[0]\n",
    "    index_fin = section[1]\n",
    "\n",
    "    indices = np.arange(index_deb, index_fin)\n",
    "    zone_len = len(indices)\n",
    "\n",
    "    dKAMP = np.zeros((Nmonth, zone_len))\n",
    "    dTWL = np.zeros((Nmonth, zone_len))\n",
    "    Ls = np.zeros((Nmonth, zone_len))\n",
    "    alpha = np.zeros((Nmonth, zone_len))\n",
    "    normal_finale = np.zeros((Nmonth, zone_len))\n",
    "    angle_inci_test = np.zeros((Nmonth, zone_len))\n",
    "\n",
    "    dx_CS_Hydro = np.zeros((Nmonth, zone_len))\n",
    "    dx_CS_MorphoLST = np.zeros((Nmonth, zone_len))\n",
    "    dx_CS_MorphoTOT = np.zeros((Nmonth, zone_len))\n",
    "    dx_CS_MorphoRIVER = np.zeros((Nmonth, zone_len))\n",
    "    dx_CS_MorphoXshore = np.zeros((Nmonth, zone_len))\n",
    "    dx_CS_total = np.zeros((Nmonth, zone_len))\n",
    "\n",
    "    X_CS_LST = np.zeros((Nmonth, zone_len))\n",
    "    X_CS_River = np.zeros((Nmonth, zone_len))\n",
    "    X_CS_MorphoTOT = np.zeros((Nmonth, zone_len))\n",
    "    X_CS_Hydro = np.zeros((Nmonth, zone_len))\n",
    "    X_CS_Xshore = np.zeros((Nmonth, zone_len))\n",
    "    X_CS_TOTAL = np.zeros((Nmonth, zone_len))\n",
    "\n",
    "    Lon = np.zeros((Nmonth, zone_len), dtype=np.float64)\n",
    "    Lat = np.zeros((Nmonth, zone_len), dtype=np.float64)\n",
    "    Lon_m = np.zeros((Nmonth, zone_len), dtype=np.float64)\n",
    "    Lat_m = np.zeros((Nmonth, zone_len), dtype=np.float64)\n",
    "    DLon = np.zeros((Nmonth, zone_len))\n",
    "    DLat = np.zeros((Nmonth, zone_len))\n",
    "    dLon = np.zeros((Nmonth, zone_len))\n",
    "    dLat = np.zeros((Nmonth, zone_len))\n",
    "\n",
    "        # Calcul du point central géographique\n",
    "    lon_mid = np.mean(lon[indices])\n",
    "    lat_mid = np.mean(lat[indices])\n",
    "\n",
    "        # INITIALISATION DU PREMIER PAS DE TEMPS\n",
    "    Lon[0, :] = lon[indices]\n",
    "    Lat[0, :] = lat[indices]\n",
    "\n",
    "    alpha[0, :-1] = calculer_azimut(Lat[0, :], Lon[0, :])\n",
    "    alpha[0, -1] = alpha[0, -2]\n",
    "\n",
    "    normal_finale[0, :] = calculer_normale_orientee(alpha[0, :], Dir[0, indices])\n",
    "\n",
    "    Lon_m[0, :], Lat_m[0, :] = lonlat2xy(Lon[0, :], Lat[0, :], lon_mid, lat_mid)\n",
    "\n",
    "    dLon[0, :-1] = np.diff(Lon_m[0, :])\n",
    "    dLat[0, :-1] = np.diff(Lat_m[0, :])\n",
    "    dLon[0, -1] = 0\n",
    "    dLat[0, -1] = 0\n",
    "\n",
    "    Ls[0, :] = np.sqrt(dLon[0, :] ** 2 + dLat[0, :] ** 2)\n",
    "\n",
    "\n",
    "        # Boucle principale de simulation\n",
    "    for t in range(1, Nmonth):\n",
    "        Lon[t, :] = Lon[t - 1, :]\n",
    "        Lat[t, :] = Lat[t - 1, :]\n",
    "\n",
    "        alpha[t, :-1] = calculer_azimut(Lat[t, :], Lon[t, :])\n",
    "        alpha[t, -1] = alpha[t, -2]\n",
    "\n",
    "        normal_finale[t, :] = calculer_normale_orientee(alpha[t, :], Dir[t, indices])\n",
    "\n",
    "        Lon_m[t, :], Lat_m[t, :] = lonlat2xy(Lon[t, :], Lat[t, :], lon_mid, lat_mid)\n",
    "\n",
    "        angle_inci_test[t, :] = (np.radians(Dir[t, indices]) - normal_finale[t, :])\n",
    "\n",
    "        dLon[t, :-1] = np.diff(Lon_m[t, :])\n",
    "        dLat[t, :-1] = np.diff(Lat_m[t, :])\n",
    "\n",
    "        Ls[t, :-1] = np.sqrt(dLon[t, :-1] ** 2 + dLat[t, :-1] ** 2)\n",
    "\n",
    "        smooth_Ls = 1.5 * Ls[t, :]  # m\n",
    "\n",
    "        for j in range(zone_len - 1):\n",
    "            idx = indices[j]\n",
    " \n",
    "                # CALCUL COMPOSANTE HYDRO - LOCAL\n",
    "            dTWL[t, j] = TWL[t, idx] - TWL[t - 1, idx]\n",
    "\n",
    "            dx_CS_Hydro[t, j] = - (TWL[t, idx] - TWL[t - 1, idx]) / (best_c_values[idx] * np.tan(beta[t, idx]))  # m\n",
    "\n",
    "                # Calcul morpho-local\n",
    "            angle_loc_rad_ip1 = angle_inci_test[t, j + 1]\n",
    "            angle_loc_rad_i = angle_inci_test[t, j]  # radians\n",
    "\n",
    "            KAMP_mass_i = 2.33 * (rohs / (rohs - roh)) * (Tp[t, idx] ** 1.5) * (best_c_values[idx] * np.tan(beta[t, idx]) ** 0.75) * (d50 ** -0.25) * (Hs[t, idx] ** 2) * abs(np.sin(2 * angle_loc_rad_i)) ** 0.6 * np.sign(angle_loc_rad_i)\n",
    "            KAMP_i = 86400 * 30 * (KAMP_mass_i / (rohs - roh)) / (1.0 - poro)  # m3/month\n",
    "\n",
    "            KAMP_mass_ip1 = 2.33 * (rohs / (rohs - roh)) * (Tp[t, idx + 1] ** 1.5) * (best_c_values[idx] * np.tan(beta[t, idx + 1]) ** 0.75) * (d50 ** -0.25) * (Hs[t, idx + 1] ** 2) * abs(np.sin(2 * angle_loc_rad_ip1)) ** 0.6 * np.sign(angle_loc_rad_ip1)\n",
    "            KAMP_ip1 = 86400 * 30 * (KAMP_mass_ip1 / (rohs - roh)) / (1.0 - poro)  # m3/month\n",
    "\n",
    "            DKAMP = KAMP_ip1 - KAMP_i\n",
    "            dKAMP[t, j] = DKAMP\n",
    "\n",
    "            # LES delta calculé pour chaque composant\n",
    "            dx_CS_MorphoTOT[t, j] = ((-1 / DoC) * ((DKAMP + QrivD[t, idx]) / Ls[t, j]) - ((1 / (best_c_values[idx]* np.tan(beta[t, idx]))) - (1 / (best_c_values[idx] * np.tan(beta[t - 1, idx]))))) * dt\n",
    "            dx_CS_MorphoLST[t, j] = ((-1 / DoC) * ((DKAMP + QrivD[t, idx]) / Ls[t, j])) * dt\n",
    "            dx_CS_MorphoXshore[t, j] = -((1 / (best_c_values[idx] * np.tan(beta[t,idx]))) - (1 / (best_c_values[idx] * np.tan(beta[t - 1, idx])))) * dt\n",
    "            dx_CS_MorphoRIVER[t, j] = ((-1 / DoC) * (QrivD[t, idx] / Ls[t, j])) * dt\n",
    "            \n",
    "            dx_CS_total[t,j]        = dx_CS_Hydro[t,j] + dx_CS_MorphoTOT[t,j]\n",
    "\n",
    "            # AJOUTS A LA POSITION DES SECTIONS\n",
    "\n",
    "        X_CS_Hydro[t, :] = X_CS_Hydro[t - 1, :] + dx_CS_Hydro[t, :]\n",
    "        X_CS_MorphoTOT[t, :] = X_CS_MorphoTOT[t - 1, :] + dx_CS_MorphoTOT[t, :]\n",
    "        X_CS_LST[t, :] = X_CS_LST[t - 1, :] + dx_CS_MorphoLST[t, :]\n",
    "        X_CS_Xshore[t, :] = X_CS_Xshore[t - 1, :] + dx_CS_MorphoXshore[t, :]\n",
    "        X_CS_River[t, :] = X_CS_River[t - 1, :] + dx_CS_MorphoRIVER[t, :]\n",
    "\n",
    "        X_CS_TOTAL[t,:]  = X_CS_TOTAL[t-1,:] + dx_CS_total[t,:]\n",
    "        \n",
    "        DLon[t,:] = - dx_CS_total[t,:]  * np.sin(normal_finale[t,:])\n",
    "        DLat[t,:] =   dx_CS_total[t,:]  * np.cos(normal_finale[t,:])\n",
    "\n",
    "        DLon[t,:] = FUNC.Wfilter(DLon[t,:],Lon_m[t,:],Lat_m[t,:],smooth_Ls)\n",
    "        DLat[t,:] = FUNC.Wfilter(DLat[t,:],Lon_m[t,:],Lat_m[t,:],smooth_Ls)\n",
    "        \n",
    "        Lon_m[t,:] = Lon_m[t,:] + DLon[t,:]\n",
    "        Lat_m[t,:] = Lat_m[t,:] + DLat[t,:]\n",
    "              \n",
    "        Lon[t,:],Lat[t,:] = xy2lonlat(Lon_m[t,:], Lat_m[t,:], lon_mid, lat_mid)\n",
    "\n",
    "    r_indices.append(indices)\n",
    "    r_dKAMP.append(dKAMP)\n",
    "    r_dTWL.append(dTWL)\n",
    "    r_Ls.append(Ls)\n",
    "    r_alpha.append(alpha)\n",
    "    r_normal_finale.append(normal_finale)\n",
    "    r_angle_inci_test.append(angle_inci_test)\n",
    "    \n",
    "    r_dx_CS_Hydro.append(dx_CS_Hydro)\n",
    "    r_dx_CS_MorphoLST.append(dx_CS_MorphoLST)\n",
    "    r_dx_CS_MorphoTOT.append(dx_CS_MorphoTOT)\n",
    "    r_dx_CS_MorphoXshore.append(dx_CS_MorphoXshore)\n",
    "    r_dx_CS_total.append(dx_CS_total)\n",
    "    \n",
    "    r_X_CS_LST.append(X_CS_LST)\n",
    "    r_X_CS_River.append(X_CS_River)\n",
    "    r_X_CS_MorphoTOT.append(X_CS_MorphoTOT)\n",
    "    r_X_CS_Hydro.append(X_CS_Hydro)\n",
    "    r_X_CS_Xshore.append(X_CS_Xshore)\n",
    "    r_X_CS_total.append(X_CS_TOTAL)\n",
    "\n",
    "    r_Lon.append(Lon)\n",
    "    r_Lat.append(Lat)\n",
    "    r_Lon_m.append(Lon_m)\n",
    "    r_Lat_m.append(Lat_m)\n",
    "    r_DLon.append(DLon)\n",
    "    r_DLat.append(DLat)\n",
    "    r_dLon.append(dLon)\n",
    "    r_dLat.append(dLat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e11540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les listes de résultats en tableaux numpy \n",
    "results_dKAMP              = np.concatenate(r_dKAMP, axis=1)\n",
    "results_dTWL               = np.concatenate(r_dTWL, axis=1)\n",
    "results_Ls                 = np.concatenate(r_Ls, axis=1)\n",
    "results_alpha              = np.concatenate(r_alpha, axis=1)\n",
    "\n",
    "results_normal_finale      = np.concatenate(r_normal_finale, axis=1)\n",
    "results_angle_inci_test    = np.concatenate(r_angle_inci_test, axis=1)\n",
    "\n",
    "results_dx_CS_Hydro        = np.concatenate(r_dx_CS_Hydro, axis=1)\n",
    "results_dx_CS_MorphoLST    = np.concatenate(r_dx_CS_MorphoLST, axis=1)\n",
    "results_dx_CS_MorphoTOT    = np.concatenate(r_dx_CS_MorphoTOT, axis=1)\n",
    "results_dx_CS_MorphoXshore = np.concatenate(r_dx_CS_MorphoXshore, axis=1)\n",
    "results_dx_CS_total        = np.concatenate(r_dx_CS_total, axis=1)\n",
    "\n",
    "results_X_CS_LST           = np.concatenate(r_X_CS_LST, axis=1)\n",
    "results_X_CS_River         = np.concatenate(r_X_CS_River, axis=1)\n",
    "results_X_CS_MorphoTOT     = np.concatenate(r_X_CS_MorphoTOT, axis=1)\n",
    "results_X_CS_Hydro         = np.concatenate(r_X_CS_Hydro, axis=1)\n",
    "results_X_CS_Xshore        = np.concatenate(r_X_CS_Xshore, axis=1)\n",
    "results_X_CS_total         = np.concatenate(r_X_CS_total, axis=1)\n",
    "  \n",
    "results_Lon                = np.concatenate(r_Lon, axis=1)\n",
    "results_Lat                = np.concatenate(r_Lat, axis=1)\n",
    "results_Lon_m              = np.concatenate(r_Lat_m, axis=1)\n",
    "results_DLon               = np.concatenate(r_DLon, axis=1)\n",
    "results_DLat               = np.concatenate(r_DLat, axis=1)\n",
    "results_dLon               = np.concatenate(r_dLon, axis=1)\n",
    "results_dLat               = np.concatenate(r_dLat, axis=1)\n",
    "results_indices            = np.concatenate(r_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84df71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## detrend mes signaux avec detrend\n",
    "Total_detrend  = np.apply_along_axis(detrend_linear,0,results_X_CS_total)\n",
    "Hydro_detrend  = np.apply_along_axis(detrend_linear,0,results_X_CS_Hydro)\n",
    "Morpho_detrend = np.apply_along_axis(detrend_linear,0,results_X_CS_LST)\n",
    "Xshores_detrend = np.apply_along_axis(detrend_linear,0,results_X_CS_Xshore)\n",
    "val_detrend   = np.apply_along_axis(detrend_linear,0,Xshores_VALIDATION)\n",
    "\n",
    "##smooth sur trois mois\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Define the sigma (standard deviation) for Gaussian smoothing\n",
    "# Assuming an approximate 30-day window, adjust sigma based on your data frequency\n",
    "sigma = 1.0  # This will smooth over approximately 3 months, adjust as needed\n",
    "\n",
    "def gaussian_smooth(data, sigma):\n",
    "    smoothed_data = np.apply_along_axis(lambda m: gaussian_filter1d(m, sigma=sigma), axis=0, arr=data)\n",
    "    return smoothed_data\n",
    "\n",
    "Total_detrend2 = gaussian_smooth(Total_detrend, sigma)\n",
    "Hydro_detrend2 = gaussian_smooth(Hydro_detrend, sigma)\n",
    "Morpho_detrend2 = gaussian_smooth(Morpho_detrend, sigma)\n",
    "Xshores_detrend2 = gaussian_smooth(Xshores_detrend, sigma)\n",
    "\n",
    "val_detrend2   = gaussian_smooth(val_detrend, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = val_detrend2[:,results_indices]\n",
    "lonX_set       = lonX[results_indices].flatten()\n",
    "latX_set       = latX[results_indices].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_detrend2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8be6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for i in range(len(results_Lon[0])):\n",
    "    # Extract data columns\n",
    "    detrend_data = Total_detrend2[:, i]\n",
    "    validation_data = validation_set[:, i]\n",
    "    \n",
    "    correlation = pearsonr(detrend_data, validation_data)\n",
    "\n",
    "    # Append results to lists\n",
    "    correlations.append(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c173d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la figure et de l'axe avec projection\n",
    "fig, ax1 = plt.subplots(figsize=(14, 15), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajout des caractéristiques de la carte\n",
    "ax1.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax1.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax1.set_extent([-180, 180, -65, 65], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax1.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax1.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Scatter plot\n",
    "sc1 = ax1.scatter(results_Lon[0], results_Lat[0], c=correlations, s=10, cmap='RdBu', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc1, ax=ax1, orientation='vertical', fraction=0.017, pad=0.04, shrink=0.8)\n",
    "cbar.set_label('Correlation', fontsize=16)\n",
    "\n",
    "# Ajustement de la taille des labels de la colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul relative deviation \n",
    "ecart = validation_set - Total_detrend2\n",
    "ampli_obs = 4*np.std(validation_set,axis=0)\n",
    "\n",
    "ecart_tot = (std(ecart,axis=0)/ampli_obs)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ae0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la figure et de l'axe avec projection\n",
    "fig, ax1 = plt.subplots(figsize=(14, 15), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajout des caractéristiques de la carte\n",
    "ax1.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax1.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax1.set_extent([-180, 180, -65, 65], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax1.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax1.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Scatter plot\n",
    "sc1 = ax1.scatter(results_Lon[0], results_Lat[0], c=ecart_tot, cmap='Blues',vmin=np.percentile(ecart_tot,5),vmax=np.percentile(ecart_tot,95), s=10,transform=ccrs.PlateCarree())\n",
    "\n",
    "# Ajout d'une colorbar\n",
    "cbar = plt.colorbar(sc1, ax=ax1, orientation='vertical', fraction=0.017, pad=0.04, shrink=0.8)\n",
    "cbar.set_label('Relative deviation [%]', fontsize=16)\n",
    "\n",
    "# Ajustement de la taille des labels de la colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638b702",
   "metadata": {},
   "source": [
    "## DETERMINATION DES ZONES IPCC sur nos données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc12e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"C:/Users/arias/Downloads/referenceRegions/referenceRegions.shp\"\n",
    "regions = gpd.read_file(shapefile_path)\n",
    "# Créer un DataFrame avec les coordonnées des points\n",
    "points_df = pd.DataFrame({\n",
    "    'lon': lonX_set,  # Liste de longitudes\n",
    "    'lat': latX_set\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a05ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer un GeoDataFrame pour les points avec un index supplémentaire\n",
    "points_df['index'] = points_df.index\n",
    "geometry = [Point(xy) for xy in zip(points_df.lon, points_df.lat)]\n",
    "points_gdf = gpd.GeoDataFrame(points_df, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Reprojeter les régions en WGS84 si nécessaire\n",
    "if regions.crs is not None and regions.crs.to_string() != 'EPSG:4326':\n",
    "    regions = regions.to_crs(epsg=4326)\n",
    "\n",
    "# Définir une fonction pour vérifier si un polygone contient des points\n",
    "def polygon_contains_points(polygon, points_gdf):\n",
    "    return points_gdf[points_gdf.geometry.within(polygon)].shape[0] > 0\n",
    "\n",
    "# Appliquer la fonction pour filtrer les régions\n",
    "filtered_regions = regions[regions.geometry.apply(lambda x: polygon_contains_points(x, points_gdf))]\n",
    "\n",
    "# Optionnel : Affiner le filtrage basé sur les limites de latitude\n",
    "regions_filtered = filtered_regions[(filtered_regions.bounds.miny >= -70) & (filtered_regions.bounds.maxy <= 70)]\n",
    "\n",
    "# Créer une colonne 'polygon_id' pour identifier chaque polygone\n",
    "regions_filtered = regions_filtered.copy()  # Créer une copie pour éviter les warnings\n",
    "regions_filtered['polygon_id'] = regions_filtered.index\n",
    "\n",
    "# Définir le nombre de couleurs nécessaires\n",
    "num_polygons = len(regions_filtered)\n",
    "cmap = plt.get_cmap('tab20')  # Vous pouvez essayer 'tab20b', 'tab20c', 'viridis', etc.\n",
    "colors = [cmap(i / num_polygons) for i in range(num_polygons)]\n",
    "\n",
    "regions_filtered['color'] = [colors[i % len(colors)] for i in range(num_polygons)]\n",
    "\n",
    "# Effectuer la jointure spatiale entre les points et les polygones filtrés\n",
    "points_with_zones = gpd.sjoin(points_gdf, regions_filtered, how='left', predicate='within')\n",
    "\n",
    "# Filtrer les points qui n'ont pas été assignés à un polygone\n",
    "points_with_zones = points_with_zones.dropna(subset=['polygon_id'])\n",
    "\n",
    "# Assigner la couleur des points en fonction du polygone auquel ils appartiennent\n",
    "points_with_zones['color'] = points_with_zones['polygon_id'].map(regions_filtered.set_index('polygon_id')['color'])\n",
    "\n",
    "# Préparer les données pour la légende\n",
    "legend_data = regions_filtered[['polygon_id', 'color', 'NAME']].drop_duplicates().sort_values(by='polygon_id')\n",
    "\n",
    "# Ajouter la colonne des indices correspondant dans points_df\n",
    "points_with_zones['original_index'] = points_with_zones['index']\n",
    "\n",
    "# Tracer les polygones filtrés\n",
    "fig, ax = plt.subplots(figsize=(14, 50), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajouter les caractéristiques de la carte\n",
    "ax.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
    "\n",
    "# Tracer les polygones filtrés avec les couleurs de contour\n",
    "for _, row in regions_filtered.iterrows():\n",
    "    ax.add_patch(plt.Polygon(list(row['geometry'].exterior.coords), closed=True, edgecolor='black', facecolor='none', linewidth=2))\n",
    "\n",
    "# Tracer les points avec les couleurs associées\n",
    "points_with_zones.plot(ax=ax, color=points_with_zones['color'], markersize=10, alpha=0.75)\n",
    "\n",
    "# # Ajouter la légende avec la taille de police ajustée\n",
    "# patches_list = [patches.Patch(color=row['color'], label=row['NAME']) for _, row in legend_data.iterrows()]\n",
    "# ax.legend(handles=patches_list, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=16)\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax.set_extent([-180, 180, -65, 65], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Afficher la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f99214",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_index = points_with_zones['index'].tolist()\n",
    "liste_polygone = points_with_zones['polygon_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Je selectionne les outputs\n",
    "Xshore_IPCC =Xshores_detrend2[:,liste_index]\n",
    "Morpho_IPCC = Morpho_detrend2[:,liste_index]\n",
    "Hydro_IPCC  = Hydro_detrend2[:,liste_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les zones uniques\n",
    "unique_zones = np.unique(liste_polygone)\n",
    "\n",
    "# Initialiser un tableau pour les moyennes par zone\n",
    "Xshore_IPCC_means = np.zeros((Xshore_IPCC.shape[0], len(unique_zones)))\n",
    "Hydro_IPCC_means = np.zeros((Hydro_IPCC.shape[0], len(unique_zones)))\n",
    "Morpho_IPCC_means = np.zeros((Morpho_IPCC.shape[0], len(unique_zones)))\n",
    "\n",
    "# Calculer la moyenne pour chaque zone\n",
    "for i, zone in enumerate(unique_zones):\n",
    "    # Trouver les indices des positions correspondant à cette zone\n",
    "    indices = np.where(liste_polygone == zone)[0]\n",
    "    # Calculer la moyenne des valeurs de X pour cette zone\n",
    "    Xshore_IPCC_means[:, i] = np.mean(Xshore_IPCC[:, indices], axis=1)\n",
    "    Hydro_IPCC_means[:, i] = np.mean(Hydro_IPCC[:, indices], axis=1)\n",
    "    Morpho_IPCC_means[:, i] = np.mean(abs(Morpho_IPCC[:, indices]), axis=1)\n",
    "\n",
    "print(Xshore_IPCC_means.shape)\n",
    "print(Hydro_IPCC_means.shape)\n",
    "print(Morpho_IPCC_means.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xshore_IPCC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a29c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xshores_IPCC_std = np.nanstd(Xshore_IPCC,axis=0)\n",
    "Morpho_IPCC_std = np.nanstd(Morpho_IPCC,axis=0)\n",
    "Hydro_IPCC_std = np.nanstd(Hydro_IPCC,axis=0)\n",
    "\n",
    "print(Xshores_IPCC_std.shape)\n",
    "print(Morpho_IPCC_std)\n",
    "print(Hydro_IPCC_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998deb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "HXM = Xshores_IPCC_std + Morpho_IPCC_std + Hydro_IPCC_std\n",
    "print(HXM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ef71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions_Hydro = (Hydro_IPCC_std/HXM)*100\n",
    "contributions_Morpho = (Morpho_IPCC_std/HXM)*100\n",
    "contributions_Xshore = (Xshores_IPCC_std/HXM)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contributions_Hydro = []\n",
    "# contributions_Morpho = []\n",
    "# contributions_Xshore = []\n",
    "\n",
    "# for i in range(len(Xshores_IPCC_std)):\n",
    "#     c_Hydro  = (Hydro_IPCC_std[i] / HXM[i]) * 100\n",
    "#     c_Morpho = (Morpho_IPCC_std[i] / HXM[i]) * 100\n",
    "#     c_Xshore = (Xshores_IPCC_std[i] / HXM[i]) * 100\n",
    "    \n",
    "#     contributions_Hydro.append(c_Hydro)\n",
    "#     contributions_Morpho.append(abs(c_Morpho))\n",
    "#     contributions_Xshore.append(c_Xshore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df53815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contributions_Hydro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions_df = pd.DataFrame({\n",
    "#     'polygone_id': unique_zones,\n",
    "    'liste_polyg' : liste_polygone,\n",
    "    'Cross-shore': contributions_Xshore,\n",
    "    'Hydrological': contributions_Hydro,\n",
    "    'Alongshore': abs(contributions_Morpho)\n",
    "}).set_index('liste_polyg')\n",
    "\n",
    "mean_contributions_df = contributions_df.groupby('liste_polyg').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour tracer les barres\n",
    "def plot_bar_plot(ax, lon, lat, contributions, color_map, **kwargs):\n",
    "    bar_width = 4\n",
    "    spacing = bar_width * 1.5\n",
    "\n",
    "    for i, (contribution_type, value) in enumerate(contributions.items()):\n",
    "        color = color_map.get(contribution_type, 'grey')\n",
    "        if isinstance(value, (int, float)):\n",
    "            # Position des barres, légèrement décalée pour éviter les chevauchements\n",
    "            x_pos = lon + (i - 1) * spacing\n",
    "            y_pos = lat\n",
    "            # Tracer chaque barre\n",
    "            rect = patches.Rectangle(\n",
    "                (x_pos, y_pos),  # Position\n",
    "                bar_width,  # Largeur\n",
    "                value / 5,  # Hauteur\n",
    "                linewidth=1,  # Largeur du contour\n",
    "                edgecolor='black',  # Couleur du contour\n",
    "                facecolor=color,  # Couleur de la barre\n",
    "                **kwargs\n",
    "            )\n",
    "            ax.add_patch(rect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_reference_bar(ax):\n",
    "    # Position de la barre (bas gauche)\n",
    "    x_pos = -170\n",
    "    y_pos = -65\n",
    "    bar_width = 4\n",
    "    bar_height = 20  # Représente le 100%\n",
    "\n",
    "    # Tracer la barre\n",
    "    rect = patches.Rectangle(\n",
    "        (x_pos, y_pos),  # Position\n",
    "        bar_width,  # Largeur\n",
    "        bar_height,  # Hauteur\n",
    "        linewidth=1,  # Largeur du contour\n",
    "        edgecolor='black',  # Couleur du contour\n",
    "        facecolor='grey',  # Couleur de la barre\n",
    "        alpha=0.6  # Transparence\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x_pos + bar_width/2, y_pos + bar_height + 2, '100%', ha='center', fontsize=16)\n",
    "\n",
    "# Ajouter une légende pour les barres\n",
    "def add_legend(ax, color_map):\n",
    "    x_legend=0.25\n",
    "    y_legend=0.25\n",
    "    legend_patches = [mpatches.Patch(color=color, label=label) for label, color in color_map.items()]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(x_legend,y_legend), fontsize=16)\n",
    "\n",
    "# Créer la carte\n",
    "fig, ax = plt.subplots(figsize=(14, 50), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Ajouter les caractéristiques de la carte\n",
    "ax.add_feature(cfeature.LAND, facecolor='gainsboro')\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black', alpha=0.9)\n",
    "\n",
    "# Tracer les points avec les couleurs associées\n",
    "points_with_zones.plot(ax=ax, color=points_with_zones['color'], markersize=10, alpha=1)\n",
    "\n",
    "# Tracer les polygones filtrés avec les couleurs de contour\n",
    "for _, row in regions_filtered.iterrows():\n",
    "    ax.add_patch(plt.Polygon(list(row['geometry'].exterior.coords), closed=True, edgecolor=row['color'], facecolor='none', linewidth=2))\n",
    "\n",
    "# Tracer les barres pour chaque zone\n",
    "for _, row in regions_filtered.iterrows():\n",
    "    lon = row.geometry.centroid.x - 3\n",
    "    lat = row.geometry.centroid.y\n",
    "    # Accéder aux contributions en utilisant l'index\n",
    "    contributions = mean_contributions_df.loc[row.name]\n",
    "    color_map = {'Cross-shore': 'orange', 'Hydrological': 'blue', 'Alongshore': 'green'}\n",
    "    plot_bar_plot(ax, lon, lat, contributions, color_map, zorder=1001)\n",
    "\n",
    "# Tracer la barre de référence\n",
    "plot_reference_bar(ax)\n",
    "\n",
    "# Ajouter la légende\n",
    "add_legend(ax, color_map)\n",
    "\n",
    "# Configurer les limites de la carte\n",
    "ax.set_extent([-180, 180, -70, 75], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Configurer les étiquettes de longitude et latitude\n",
    "ax.set_xticks([-180, -120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\n",
    "ax.set_yticks([-60, -30, 0, 30, 60], crs=ccrs.PlateCarree())\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"W\" if val < 0 else \"E\"}'))\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, pos: f'{abs(val)}°{\"S\" if val < 0 else \"N\"}'))\n",
    "\n",
    "# Taille de police pour les labels des axes\n",
    "ax.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Afficher la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618668b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d53fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53397b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e126d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
